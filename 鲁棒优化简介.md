# 鲁棒优化简介（Introduction）

汤勤深

根据维基百科，鲁棒优化（Robust Optimization）是最优化理论中的一类用来寻求在不确定（Uncertain)环境中使优化问题具有一定程度的鲁棒性（Robustness）的方法。其中，不确定性可以通过问题的参数或者解的确定性变异（Deterministic variability）来刻画 [(Wikipedia, 2021)](https://en.wikipedia.org/wiki/Robust_optimization)。也就是说鲁棒优化是用来寻求对不确定性免疫的解的一类方法 [(Bertsimas and Sim, 2004)](https://pubsonline.informs.org/doi/abs/10.1287/opre.1030.0065?journalCode=opre). 

在传统的优化模型中，通常假设模型的输入数据是具体的、准确的数值。然而，现实生活中所获得的大部分数据都是具有一定的误差。而有时细微的误差也将导致优化问题的最优解不再最优或者导致原问题不具有可行解（Infeasible）。以[NETLIB](http://www.netlib.org/)中题PILOT4为例，其某一约束为：
$$
\begin{align\*}
    {\pmb{a}^{\top} \pmb{x} \equiv} & {-15.79081 x_{826}-8.598819 x_{827}-1.88789 x_{828}-1.362417 x_{829}-1.526049 x_{830}}\\\\\\
    & { -0.031883 x_{849}-28.725555 x_{850}-10.792065 x_{851}-0.19004 x_{852}-2.757176 x_{853}}\\\\\\
    & { -12.290832 x_{854}+717.562256 x_{855}-0.057865 x_{856}-3.785417 x_{857}-78.30661 x_{858}} \\\\\\
    & { -122.163055 x_{859}-6.46609 x_{860}-0.48371 x_{861}-0.615264 x_{862}-1.353783 x_{863}}\\\\\\
    & { -84.644257 x_{864}-122.459045 x_{865}-43.15593 x_{866}-1.712592 x_{870}-0.401597 x_{871}}\\\\\\ 
    & { +x_{880}-0.946049 x_{898}-0.946049 x_{916} \geq b \equiv 23.387405.}
\end{align\*}
$$
用Cplex解得这个问题的解为：
$$
\begin{array}{lll}
x_{826}^{\*} = 255.6112787181108 & x_{827}^{\*}=6240.488912232100 & x_{828}^{\*}=3624.613324098961 \\\\\\ 
{x_{829}^{\*} = 18.20205065283259} & {x_{849}^{\*}=174397.0389573037} & {x_{870}^{\*}=14250.00176680900} \\\\\\
{x_{871}^{\*} = 25910.00731692178} &  {x_{880}^{\*}=104958.3199274139} &.
\end{array}
$$

解$\pmb{x}^{\*}$满足$\pmb a^{\top} \pmb x^\* = b$。 然而，只要稍微改动$\pmb{a}$中某一项的系数，那么$\pmb a^{\top} \pmb x^\* \neq b$，也即$\pmb x^\*$不再是最优解。

如何解决这个问题？最直观的方法是，假设每一个系数都在一定范围内变动，从而求一个解使得对所有在这个范围内变动的系数都是最优的。举个例子，对于约束
$$
ax \leq b,
$$
我们希望它对所有$a \in [\underline{a}, \bar{a}]$ 都成立，也即原有约束将变成
$$
ax \leq b,\, \forall a \in [\underline{a}, \bar{a}].
$$
这个问题首先由Soyster在1973年研究[(Soyster, 1973)](https://pubsonline.informs.org/doi/abs/10.1287/opre.21.5.1154)。他假设系数矩阵$\pmb A$中每一列都在一个下凸集（Convex set）中（也称为列不确定性），也即他研究如下问题： 
$$
\begin{array}{cl} 
    \max & \pmb{c}^{\prime} \pmb{x} \\\\\\ 
    \mathbf { s.t. } & \displaystyle \sum_{j=1}^{N} \pmb A_{j} x_{j} \leq \pmb{b}, \quad \forall \pmb A_{j} \in \mathcal{U}\_{j}, j\in [N] \\\\\\ 
     & \pmb{x} \geq \mathbf{0}.
\end{array}
$$

然而，此种方法所得到的解太过于保守（Conservative）而饱受诟病。随后Ben-tal and Nemirovski [1998](https://pubsonline.informs.org/doi/10.1287/moor.23.4.769), [1999](https://www.sciencedirect.com/science/article/abs/pii/S0167637799000164), [2000](https://link.springer.com/article/10.1007/PL00011380)和[EI-Ghaoui and Lebret 1997](https://epubs.siam.org/doi/10.1137/S0895479896298130), [EI-Ghaoui et al. 1998]()为降低保守性而引进了椭球型不确定集（Ellipsoids uncertainty set）。同时也考虑了其他形式的不确定性，比如行不确定性。其中，椭球型不确定集一方面比较难跟现实数据结合，另外一方面，转化（Reformulate）之后的模型大都是二阶锥规划（Second order cone programming）或者半正定规划（Semi-definite programming）问题---求解起来比较复杂。此外，这种方法依然比较保守。

2004年， [Bertsimas and Sim 2004](https://pubsonline.informs.org/doi/abs/10.1287/opre.1030.0065?journalCode=opre)为了克服椭球型不确定集的缺点，引进了预算不确定集（budget uncertainty set），将原问题转化成线性规划问题，并且得出了所得解可行概率的下限，也即所得解对所有约束不可行的概率的上限。

本电子书将在第三章介绍不确定性最优化，不同种类的不确定集，以及鲁棒优化理论中很核心的对等式转换理论（Robust counterpart）。此电子书把这种将模型参数假定在给定不确定集中而进行优化问题求解的方法统称为经典鲁棒优化（Classical robust optimization）。

为什么称为经典鲁棒优化？因为经典鲁棒优化构成了现在普遍使用的分布鲁棒优化（Distributionally robust optimization）的基础，同时也是分布鲁棒优化的一种特殊形式。

如果我们不仅仅知道优化模型某些参数（以下称为随机量，即random variable）的支撑集（Support set），还知道这些参数服从某一分布，那此类优化问题为随机优化问题（Stochastic programming）：

$$
\begin{align}
    \min\ & \mathbb{E}\_{\mathbb{P}}[g(\pmb x,\tilde{\pmb \xi})], \label{model::stochastic programming} \\\\\\ 
    \mathbf{s.t.}\ &  \mathbb{E}\_{\mathbb{P}}[f_i(\pmb x,\tilde{\pmb \xi})] \leq 0,\, i\in[I], \notag
\end{align}
$$

一般地，我们假设$g(\cdot)$和$f_i(\cdot)$均为下凸函数。

可是，恰恰是知道随机量服从某一分布这个假设导致很多问题。一方面，模型中的随机量在生产经营或者模型背景中通常是多种因素作用的结果，这就导致很难准确估计某一随机量的边际分布，更别说所有随机量的集中分布了。另一方面，就算知道边际分布或者集中分布，除了很多时候因为随机量过多而模型维度过大之外，在多阶段问题中，还常常受制于“维度诅咒（Curse of dimentionality）”。而在现今科技发展，各行各业所收集和掌握的数据量呈井喷式态势，使得从大量数据中提取一些随机量的统计信息（比如需求的均值和方差）变得可行。而分布式鲁棒优化提供了将这些统计信息融入到模型决策中的一种思路。

具体来说，如果我们将所需要用到的统计信息放到一个集合中，假设为$\mathcal{F}$，那么$\mathcal{F}$就是所有拥有这些统计信息的分布的一个集合。我们称这个集合为模糊集（Ambiguity set）。在模糊集中，选取一个分布使得在最坏情况下，进行模型求解。这样所求得的解就具有一定的鲁棒性。也即，模型$\eqref{model::stochastic programming}$将变成：

$$\begin{align}
    \min\ & \sup_{\mathbb{P} \in \mathcal{F}}\mathbb{E}\_{\mathbb{P}}[g(\pmb x,\tilde{\pmb\xi}], \label{model::General DRO model}\\\\\\
    \mathbf{s.t.}\ &  \sup_{\mathbb{P} \in \mathcal{F}} \mathbb{E}\_{\mathbb{P}}[f_i(\pmb x,\tilde{\pmb\xi}] \leq 0,\, i\in[I].\notag
    \end{align}
$$
我们称模型$\eqref{model::General DRO model}$为分布鲁棒优化模型。其中，如果$\mathcal{F} = \{\mathbb{P}\}$，那分布鲁棒模型就是随机优化模型。如果，在$\mathcal{F}$中，我们只知道随机量$\tilde{\pmb \xi}$在某一个集合$\mathcal{U}$中，那分布鲁棒模型退化成经典鲁棒模型。本书将在第四种着重介绍不同不同的模糊集和分布式鲁棒模型的求解方法。

以上的经典鲁棒优化模型和分布鲁棒优化模型只适用于单阶段（Single stage）问题。而现实中所面临的决策往往是多阶段（Multi-stage）的。
其中，最经典的莫过于两阶段随机规划模型：
$$\begin{align}
    \min\ & \pmb c^{\top} \pmb x + \mathbb{E}\_{\mathbb{P}}[g(\pmb x,\tilde{\pmb \xi}],\label{model::Two Stage SP model} \\\\\\
    \mathbf{s.t.}\ &  \pmb{Ax} = \pmb b\notag \\\\\\
    & \pmb x \geq \mathbf{0},\notag
    \end{align}
$$
其中
$$ \begin{align\*}
    g(\pmb x,\pmb \xi) = \min\ & \pmb q^{\top}\pmb{y},\\\\\\
    \mathbf{s.t.}\ &  \pmb{T}\pmb x + \pmb{W}\pmb y = \pmb h\\\\\\
    & \pmb y \geq \mathbf{0}，
    \end{align\*}
$$
$\pmb \xi := (\pmb q, \pmb h, \pmb T, \pmb W)$为第二阶段的输入数据。也即第二阶段的决策依赖于第一阶段随机量的实现（Realization）。这就导致在模型 $\eqref{model::Two Stage SP model}$ 中，第一阶段决策变量$\pmb x$称为“现时决策（Here-and-now decision）”各阶段之间决策变量和随机量之间的交互使得问题的复杂度随着阶段的增加而呈指数增长，也即“维度诅咒”。在随机规划中，学者们通过引入决策规则（Decision rule）去近似地解决这一问题。但是，因为近似模型表现不好而被“打入冷宫”。而鲁棒优化的出现，让决策规则重洗换发出了勃勃生机。本书将在第五章详细介绍多阶段随机规划问题以及如何使用不同的决策规则和鲁棒优化的方法对其进行近似，并且取得很好的近似效果。

近年来，随着鲁棒优化在各种不同问题求解中的良好表现，其价值越来越被学界和业界所发现。比如，从鲁棒优化的角度去处理具有广泛运用的机会约束问题，可以收到比较好的效果。而最新的研究显示，被广泛运用在机器学习中回归模型（Regression）的正则性（Regularization）和鲁棒优化具有等价关系。这一发现启发了越来越多的学者将鲁棒优化和机器学习相结合。本书将在第六章讲解鲁棒优化下的机会约束问题，在第七种讲解鲁棒优化和机器学习结合的一些最新研究成果。

同时，在本书的最后一章，我们将介绍鲁棒模型在不同求解平台上的实现方式。也将在不同的章节引入不同的案例或算例。